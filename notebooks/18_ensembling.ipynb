{
 "metadata": {
  "name": "",
  "signature": "sha256:826a58b368e44f6b660e629849bfc4b2328ba5aef63617cc98b04efc4a0f0c9c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Ensembling\n",
      "\n",
      "*Adapted from Chapter 8 of [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Part 1: Introduction\n",
      "Let's pretend that instead of building a single model to solve a classification problem, you created **five independent models**, and each model was correct about 70% of the time. If you combined these models into an \"ensemble\" and used their majority vote as a prediction, how often would the ensemble be correct?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "# set a seed for reproducibility\n",
      "np.random.seed(1234)\n",
      "\n",
      "# generate 1000 random numbers (between 0 and 1) for each model, representing 1000 observations\n",
      "mod1 = np.random.rand(1000)\n",
      "mod2 = np.random.rand(1000)\n",
      "mod3 = np.random.rand(1000)\n",
      "mod4 = np.random.rand(1000)\n",
      "mod5 = np.random.rand(1000)\n",
      "\n",
      "# each model independently predicts 1 (the \"correct response\") if random number was at least 0.3\n",
      "preds1 = np.where(mod1 > 0.3, 1, 0)\n",
      "preds2 = np.where(mod2 > 0.3, 1, 0)\n",
      "preds3 = np.where(mod3 > 0.3, 1, 0)\n",
      "preds4 = np.where(mod4 > 0.3, 1, 0)\n",
      "preds5 = np.where(mod5 > 0.3, 1, 0)\n",
      "\n",
      "# print the first 20 predictions from each model\n",
      "print preds1[:20]\n",
      "print preds2[:20]\n",
      "print preds3[:20]\n",
      "print preds4[:20]\n",
      "print preds5[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# average the predictions and then round to 0 or 1\n",
      "ensemble_preds = np.round((preds1 + preds2 + preds3 + preds4 + preds5)/5.0).astype(int)\n",
      "\n",
      "# print the ensemble's first 20 predictions\n",
      "print ensemble_preds[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# how accurate was each individual model?\n",
      "print preds1.mean()\n",
      "print preds2.mean()\n",
      "print preds3.mean()\n",
      "print preds4.mean()\n",
      "print preds5.mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# how accurate was the ensemble?\n",
      "print ensemble_preds.mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Ensemble learning (or \"ensembling\")** is the process of combining several predictive models in order to produce a combined model that is more accurate than any individual model.\n",
      "\n",
      "- **Regression:** take the average of the predictions\n",
      "- **Classification:** take a vote and use the most common prediction, or take the average of the predicted probabilities\n",
      "\n",
      "For ensembling to work well, the models must have the following characteristics:\n",
      "\n",
      "- **Accurate:** they outperform random guessing\n",
      "- **Independent:** their predictions are not correlated with one another\n",
      "\n",
      "**The big idea:** If you have a collection of individually imperfect (and independent) models, the \"one-off\" mistakes made by each model are probably not going to be made by the rest of the models, and thus the mistakes will be discarded when averaging the models.\n",
      "\n",
      "**Note:** As you add more models to the voting process, the probability of error decreases, which is known as [Condorcet's Jury Theorem](http://en.wikipedia.org/wiki/Condorcet%27s_jury_theorem)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Ensembling methods\n",
      "\n",
      "There are two basic methods for ensembling:\n",
      "\n",
      "- Use a model that ensembles for you\n",
      "- Manually ensemble your individual models\n",
      "\n",
      "What makes a good \"manual ensemble\"?\n",
      "\n",
      "- Different types of models\n",
      "- Different combinations of features\n",
      "- Different tuning parameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![Machine learning flowchart](images/crowdflower_ensembling.jpg)\n",
      "\n",
      "*Machine learning flowchart created by the [winner](https://github.com/ChenglongChen/Kaggle_CrowdFlower) of Kaggle's [CrowdFlower competition](https://www.kaggle.com/c/crowdflower-search-relevance)*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Part 2: Bagging\n",
      "\n",
      "The primary weakness of **decision trees** is that they don't tend to have the best predictive accuracy. This is partially due to **high variance**, meaning that different splits in the training data can lead to very different trees.\n",
      "\n",
      "**Bagging** is a general purpose procedure for reducing the variance of a machine learning method, but is particularly useful for decision trees. Bagging is short for **bootstrap aggregation**, meaning the aggregation of bootstrap samples.\n",
      "\n",
      "What is a **bootstrap sample**? A random sample with replacement:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# set a seed for reproducibility\n",
      "np.random.seed(1)\n",
      "\n",
      "# create an array of 1 through 20\n",
      "nums = np.arange(1, 21)\n",
      "print nums\n",
      "\n",
      "# sample that array 20 times with replacement\n",
      "print np.random.choice(a=nums, size=20, replace=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**How does bagging work (for decision trees)?**\n",
      "\n",
      "1. Grow B trees using B bootstrap samples from the training data.\n",
      "2. Train each tree on its bootstrap sample and make predictions.\n",
      "3. Combine the predictions:\n",
      "    - Average the predictions for **regression trees**\n",
      "    - Take a majority vote for **classification trees**\n",
      "\n",
      "Notes:\n",
      "\n",
      "- **Each bootstrap sample** should be the same size as the original training set.\n",
      "- **B** should be a large enough value that the error seems to have \"stabilized\".\n",
      "- The trees are **grown deep** so that they have low bias/high variance.\n",
      "\n",
      "Bagging increases predictive accuracy by **reducing the variance**, similar to how cross-validation reduces the variance associated with train/test split (for estimating out-of-sample error) by splitting many times an averaging the results."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Manually implementing bagged decision trees (with B=10)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read in and prepare the vehicle training data\n",
      "import pandas as pd\n",
      "url = 'https://raw.githubusercontent.com/justmarkham/DAT7/master/data/vehicles_train.csv'\n",
      "train = pd.read_csv(url)\n",
      "train['vtype'] = train.vtype.map({'car':0, 'truck':1})\n",
      "train"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# set a seed for reproducibility\n",
      "np.random.seed(123)\n",
      "\n",
      "# create ten bootstrap samples (will be used to select rows from the DataFrame)\n",
      "samples = [np.random.choice(a=14, size=14, replace=True) for _ in range(1, 11)]\n",
      "samples"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# show the rows for the first decision tree\n",
      "train.iloc[samples[0], :]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read in and prepare the vehicle testing data\n",
      "url = 'https://raw.githubusercontent.com/justmarkham/DAT7/master/data/vehicles_test.csv'\n",
      "test = pd.read_csv(url)\n",
      "test['vtype'] = test.vtype.map({'car':0, 'truck':1})\n",
      "test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.tree import DecisionTreeRegressor\n",
      "\n",
      "# grow each tree deep\n",
      "treereg = DecisionTreeRegressor(max_depth=None, random_state=123)\n",
      "\n",
      "# list for storing predicted price from each tree\n",
      "predictions = []\n",
      "\n",
      "# define testing data\n",
      "X_test = test.iloc[:, 1:]\n",
      "y_test = test.iloc[:, 0]\n",
      "\n",
      "# grow one tree for each bootstrap sample and make predictions on testing data\n",
      "for sample in samples:\n",
      "    X_train = train.iloc[sample, 1:]\n",
      "    y_train = train.iloc[sample, 0]\n",
      "    treereg.fit(X_train, y_train)\n",
      "    y_pred = treereg.predict(X_test)\n",
      "    predictions.append(y_pred)\n",
      "\n",
      "# convert predictions from list to NumPy array\n",
      "predictions = np.array(predictions)\n",
      "predictions"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# average predictions\n",
      "np.mean(predictions, axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# calculate RMSE\n",
      "from sklearn import metrics\n",
      "y_pred = np.mean(predictions, axis=0)\n",
      "np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Bagged decision trees in scikit-learn (with B=500)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define the training and testing sets\n",
      "X_train = train.iloc[:, 1:]\n",
      "y_train = train.iloc[:, 0]\n",
      "X_test = test.iloc[:, 1:]\n",
      "y_test = test.iloc[:, 0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# instruct BaggingRegressor to use DecisionTreeRegressor as the \"base estimator\"\n",
      "from sklearn.ensemble import BaggingRegressor\n",
      "bagreg = BaggingRegressor(DecisionTreeRegressor(), n_estimators=500, bootstrap=True, oob_score=True, random_state=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# fit and predict\n",
      "bagreg.fit(X_train, y_train)\n",
      "y_pred = bagreg.predict(X_test)\n",
      "y_pred"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# calculate RMSE\n",
      "np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Estimating out-of-sample error\n",
      "\n",
      "For bagged models, out-of-sample error can be estimated without using **train/test split** or **cross-validation**!\n",
      "\n",
      "On average, each bagged tree uses about **two-thirds** of the observations. For each tree, the **remaining observations** are called \"out-of-bag\" observations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# show the first bootstrap sample\n",
      "samples[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# show the \"in-bag\" observations for each sample\n",
      "for sample in samples:\n",
      "    print set(sample)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# show the \"out-of-bag\" observations for each sample\n",
      "for sample in samples:\n",
      "    print sorted(set(range(14)) - set(sample))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How to calculate **\"out-of-bag error\":**\n",
      "\n",
      "1. For every observation in the training data, predict its response value using **only** the trees in which that observation was out-of-bag. Average those predictions (for regression) or take a majority vote (for classification).\n",
      "2. Compare all predictions to the actual response values in order to compute the out-of-bag error.\n",
      "\n",
      "When B is sufficiently large, the **out-of-bag error** is an accurate estimate of **out-of-sample error**."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compute the out-of-bag R-squared score (not MSE, unfortunately!) for B=500\n",
      "bagreg.oob_score_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Estimating feature importance\n",
      "\n",
      "Bagging increases **predictive accuracy**, but decreases **model interpretability** because it's no longer possible to visualize the tree to understand the importance of each feature.\n",
      "\n",
      "However, we can still obtain an overall summary of **feature importance** from bagged models:\n",
      "\n",
      "- **Bagged regression trees:** calculate the total amount that **MSE** is decreased due to splits over a given feature, averaged over all trees\n",
      "- **Bagged classification trees:** calculate the total amount that **Gini index** is decreased due to splits over a given feature, averaged over all trees"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Part 3: Random Forests\n",
      "\n",
      "Random Forests is a **slight variation of bagged trees** that has even better performance:\n",
      "\n",
      "- Exactly like bagging, we create an ensemble of decision trees using bootstrapped samples of the training set.\n",
      "- However, when building each tree, each time a split is considered, a **random sample of m features** is chosen as split candidates from the **full set of p features**. The split is only allowed to use **one of those m features**.\n",
      "    - A new random sample of features is chosen for **every single tree at every single split**.\n",
      "    - For **classification**, m is typically chosen to be the square root of p.\n",
      "    - For **regression**, m is typically chosen to be somewhere between p/3 and p.\n",
      "\n",
      "What's the point?\n",
      "\n",
      "- Suppose there is **one very strong feature** in the data set. When using bagged trees, most of the trees will use that feature as the top split, resulting in an ensemble of similar trees that are **highly correlated**.\n",
      "- Averaging highly correlated quantities does not significantly reduce variance (which is the entire goal of bagging).\n",
      "- By randomly leaving out candidate features from each split, **Random Forests \"decorrelates\" the trees**, such that the averaging process can reduce the variance of the resulting model."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Part 4: Comparing Decision Trees and Random Forests"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Exploring and preparing the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Predicting salary with a decision tree\n",
      "\n",
      "Find the best **max_depth** for a decision tree using cross-validation:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# list of values to try for max_depth\n",
      "max_depth_range = range(1, 21)\n",
      "\n",
      "# list to store the average RMSE for each value of max_depth\n",
      "RMSE_scores = []\n",
      "\n",
      "# use 10-fold cross-validation with each value of max_depth\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "for depth in max_depth_range:\n",
      "    treereg = DecisionTreeRegressor(max_depth=depth, random_state=1)\n",
      "    MSE_scores = cross_val_score(treereg, X, y, cv=10, scoring='mean_squared_error')\n",
      "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))\n",
      "\n",
      "# plot max_depth (x-axis) versus RMSE (y-axis)\n",
      "import matplotlib.pyplot as plt\n",
      "plt.plot(max_depth_range, RMSE_scores)\n",
      "plt.xlabel('max_depth')\n",
      "plt.ylabel('RMSE (lower is better)')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# max_depth=2 was best, so fit a tree using that parameter\n",
      "treereg = DecisionTreeRegressor(max_depth=2, random_state=1)\n",
      "treereg.fit(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compute feature importances\n",
      "pd.DataFrame({'feature':feature_cols, 'importance':treereg.feature_importances_}).sort('importance')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Predicting salary with a Random Forest"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestRegressor\n",
      "rfreg = RandomForestRegressor()\n",
      "rfreg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One important tuning parameter is **n_estimators:** the number of trees that should be grown."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# list of values to try for n_estimators\n",
      "estimator_range = range(10, 310, 10)\n",
      "\n",
      "# list to store the average RMSE for each value of n_estimators\n",
      "RMSE_scores = []\n",
      "\n",
      "# use 5-fold cross-validation with each value of n_estimators\n",
      "for estimator in estimator_range:\n",
      "    rfreg = RandomForestRegressor(n_estimators=estimator, random_state=1)\n",
      "    MSE_scores = cross_val_score(rfreg, X, y, cv=5, scoring='mean_squared_error')\n",
      "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))\n",
      "\n",
      "# plot n_estimators (x-axis) versus RMSE (y-axis)\n",
      "plt.plot(estimator_range, RMSE_scores)\n",
      "plt.xlabel('n_estimators')\n",
      "plt.ylabel('RMSE (lower is better)')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The other important tuning parameter is **max_features:** the number of features that should be considered at each split."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# list of values to try for max_features\n",
      "feature_range = range(1, len(feature_cols)+1)\n",
      "\n",
      "# list to store the average RMSE for each value of max_features\n",
      "RMSE_scores = []\n",
      "\n",
      "# use 10-fold cross-validation with each value of max_features\n",
      "for feature in feature_range:\n",
      "    rfreg = RandomForestRegressor(n_estimators=150, max_features=feature, random_state=1)\n",
      "    MSE_scores = cross_val_score(rfreg, X, y, cv=10, scoring='mean_squared_error')\n",
      "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))\n",
      "\n",
      "# plot max_features (x-axis) versus RMSE (y-axis)\n",
      "plt.plot(feature_range, RMSE_scores)\n",
      "plt.xlabel('max_features')\n",
      "plt.ylabel('RMSE (lower is better)')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# max_features=8 was best, so fit a Random Forest using that parameter\n",
      "rfreg = RandomForestRegressor(n_estimators=150, max_features=8, oob_score=True, random_state=1)\n",
      "rfreg.fit(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compute feature importances\n",
      "pd.DataFrame({'feature':feature_cols, 'importance':rfreg.feature_importances_}).sort('importance')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compute the out-of-bag R-squared score\n",
      "rfreg.oob_score_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Reduce X to its most important features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# check the shape of X\n",
      "X.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# set a threshold for which features to include\n",
      "print rfreg.transform(X, threshold=0.1).shape\n",
      "print rfreg.transform(X, threshold='mean').shape\n",
      "print rfreg.transform(X, threshold='median').shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create a new feature matrix that only include important features\n",
      "X_important = rfreg.transform(X, threshold='mean')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# check the RMSE for a Random Forest that only uses important features\n",
      "rfreg = RandomForestRegressor(n_estimators=150, max_features=3, random_state=1)\n",
      "scores = cross_val_score(rfreg, X_important, y, cv=10, scoring='mean_squared_error')\n",
      "np.mean(np.sqrt(-scores))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Part 5: Conclusion\n",
      "\n",
      "## Comparing Random Forests with Decision Trees\n",
      "\n",
      "**Advantages of Random Forests:**\n",
      "\n",
      "- Performance is competitive with the best supervised learning methods\n",
      "- Provides a more reliable estimate of feature importance\n",
      "- Allows you to estimate out-of-sample error without using train/test split or cross-validation\n",
      "\n",
      "**Disadvantages of Random Forests:**\n",
      "\n",
      "- Less interpretable\n",
      "- Slower to train\n",
      "- Slower to predict\n",
      "\n",
      "## Comparing \"manual\" ensembling with a single model approach\n",
      "\n",
      "**Advantages of ensembling:**\n",
      "\n",
      "- Increases predictive accuracy\n",
      "- Easy to get started\n",
      "\n",
      "**Disadvantages of ensembling:**\n",
      "\n",
      "- Decreases interpretability\n",
      "- Takes longer to train\n",
      "- Takes longer to predict\n",
      "- More complex to automate and maintain\n",
      "- Small gains in accuracy may not be worth the added complexity"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "![Machine learning flowchart](images/driver_ensembling.png)\n",
      "\n",
      "*Machine learning flowchart created by the [second place finisher](http://blog.kaggle.com/2015/04/20/axa-winners-interview-learning-telematic-fingerprints-from-gps-data/) of Kaggle's [Driver Telematics competition](https://www.kaggle.com/c/axa-driver-telematics-analysis)*"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}